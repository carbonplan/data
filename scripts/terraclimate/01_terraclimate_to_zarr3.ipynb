{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# TERRACLIMATE to Zarr\n",
    "\n",
    "_by Joe Hamman (CarbonPlan), June 29, 2020_\n",
    "\n",
    "This notebook converts the raw TERAACLIMATE dataset to Zarr format.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "- inake catalog: `climate.gridmet_opendap`\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "- Cloud copy of TERRACLIMATE\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- No reprojection or processing of the data is done in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "from typing import List\n",
    "import urlpath\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "name = \"terraclimate\"\n",
    "chunks = {\"lat\": 1440, \"lon\": 1440, \"time\": 12}\n",
    "years = list(range(1958, 2020))\n",
    "cache_location = f\"gs://carbonplan-scratch/{name}-cache/\"\n",
    "target_location = f\"gs://carbonplan-data/raw/{name}/4000m/raster.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = 1\n",
    "options.worker_memory = 42\n",
    "cluster = gateway.new_cluster(cluster_options=options)\n",
    "cluster.adapt(minimum=0, maximum=40)\n",
    "client = cluster.get_client()\n",
    "cluster\n",
    "# client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "try:\n",
    "    _ = fs.rm(target_location, recursive=True)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to remove all temporary zarr stores\n",
    "zarrs = [\n",
    "    fn + \".zarr\" for fn in fs.glob(\"carbonplan-scratch/terraclimate-cache/*nc\")\n",
    "]\n",
    "fs.rm(zarrs, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"aet\",\n",
    "    \"def\",\n",
    "    \"pet\",\n",
    "    \"ppt\",\n",
    "    \"q\",\n",
    "    \"soil\",\n",
    "    \"srad\",\n",
    "    \"swe\",\n",
    "    \"tmax\",\n",
    "    \"tmin\",\n",
    "    \"vap\",\n",
    "    \"ws\",\n",
    "    \"vpd\",\n",
    "    \"PDSI\",\n",
    "]\n",
    "\n",
    "rename_vars = {\"PDSI\": \"pdsi\"}\n",
    "\n",
    "mask_opts = {\n",
    "    \"PDSI\": (\"lt\", 10),\n",
    "    \"aet\": (\"lt\", 32767),\n",
    "    \"def\": (\"lt\", 32767),\n",
    "    \"pet\": (\"lt\", 32767),\n",
    "    \"ppt\": (\"lt\", 32767),\n",
    "    \"ppt_station_influence\": None,\n",
    "    \"q\": (\"lt\", 2147483647),\n",
    "    \"soil\": (\"lt\", 32767),\n",
    "    \"srad\": (\"lt\", 32767),\n",
    "    \"swe\": (\"lt\", 10000),\n",
    "    \"tmax\": (\"lt\", 200),\n",
    "    \"tmax_station_influence\": None,\n",
    "    \"tmin\": (\"lt\", 200),\n",
    "    \"tmin_station_influence\": None,\n",
    "    \"vap\": (\"lt\", 300),\n",
    "    \"vap_station_influence\": None,\n",
    "    \"vpd\": (\"lt\", 300),\n",
    "    \"ws\": (\"lt\", 200),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(key, da):\n",
    "    \"\"\"helper function to mask DataArrays based on a threshold value\"\"\"\n",
    "    if mask_opts.get(key, None):\n",
    "        op, val = mask_opts[key]\n",
    "        if op == \"lt\":\n",
    "            da = da.where(da < val)\n",
    "        elif op == \"neq\":\n",
    "            da = da.where(da != val)\n",
    "    return da\n",
    "\n",
    "\n",
    "def preproc(ds):\n",
    "    \"\"\"custom preprocessing function for terraclimate data\"\"\"\n",
    "    rename = {}\n",
    "\n",
    "    station_influence = ds.get(\"station_influence\", None)\n",
    "\n",
    "    if station_influence is not None:\n",
    "        ds = ds.drop_vars(\"station_influence\")\n",
    "\n",
    "    var = list(ds.data_vars)[0]\n",
    "\n",
    "    if var in rename_vars:\n",
    "        rename[var] = rename_vars[var]\n",
    "\n",
    "    if \"day\" in ds.coords:\n",
    "        rename[\"day\"] = \"time\"\n",
    "\n",
    "    if station_influence is not None:\n",
    "        ds[f\"{var}_station_influence\"] = station_influence\n",
    "\n",
    "    if rename:\n",
    "        ds = ds.rename(rename)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def postproc(ds):\n",
    "    \"\"\"custom post processing function to clean up terraclimate data\"\"\"\n",
    "    drop_encoding = [\n",
    "        \"chunksizes\",\n",
    "        \"fletcher32\",\n",
    "        \"shuffle\",\n",
    "        \"zlib\",\n",
    "        \"complevel\",\n",
    "        \"dtype\",\n",
    "        \"_Unsigned\",\n",
    "        \"missing_value\",\n",
    "        \"_FillValue\",\n",
    "        \"scale_factor\",\n",
    "        \"add_offset\",\n",
    "    ]\n",
    "    for v in ds.data_vars.keys():\n",
    "        with xr.set_options(keep_attrs=True):\n",
    "            ds[v] = apply_mask(v, ds[v])\n",
    "        for k in drop_encoding:\n",
    "            ds[v].encoding.pop(k, None)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_encoding(ds):\n",
    "    compressor = Blosc()\n",
    "    encoding = {key: {\"compressor\": compressor} for key in ds.data_vars}\n",
    "    return encoding\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def download(source_url: str, cache_location: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a remote file to a cache.\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_url : str\n",
    "        Path or url to the source file.\n",
    "    cache_location : str\n",
    "        Path or url to the target location for the source file.\n",
    "    Returns\n",
    "    -------\n",
    "    target_url : str\n",
    "        Path or url in the form of `{cache_location}/hash({source_url})`.\n",
    "    \"\"\"\n",
    "    fs = fsspec.get_filesystem_class(cache_location.split(\":\")[0])(\n",
    "        token=\"cloud\"\n",
    "    )\n",
    "\n",
    "    name = urlpath.URL(source_url).name\n",
    "    target_url = os.path.join(cache_location, name)\n",
    "\n",
    "    # there is probably a better way to do caching!\n",
    "    try:\n",
    "        fs.open(target_url)\n",
    "        return target_url\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    with fsspec.open(source_url, mode=\"rb\") as source:\n",
    "        with fs.open(target_url, mode=\"wb\") as target:\n",
    "            target.write(source.read())\n",
    "    return target_url\n",
    "\n",
    "\n",
    "@dask.delayed(pure=True, traverse=False)\n",
    "def nc2zarr(source_url: str, cache_location: str) -> str:\n",
    "    \"\"\"convert netcdf data to zarr\"\"\"\n",
    "    fs = fsspec.get_filesystem_class(source_url.split(\":\")[0])(token=\"cloud\")\n",
    "    print(source_url)\n",
    "\n",
    "    target_url = source_url + \".zarr\"\n",
    "\n",
    "    if fs.exists(urlpath.URL(target_url) / \".zmetadata\"):\n",
    "        return target_url\n",
    "\n",
    "    with dask.config.set(scheduler=\"single-threaded\"):\n",
    "\n",
    "        try:\n",
    "            ds = (\n",
    "                xr.open_dataset(fs.open(source_url), engine=\"h5netcdf\")\n",
    "                .pipe(preproc)\n",
    "                .pipe(postproc)\n",
    "                .load()\n",
    "                .chunk(chunks)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(source_url)\n",
    "\n",
    "        mapper = fs.get_mapper(target_url)\n",
    "        ds.to_zarr(mapper, mode=\"w\", consolidated=True)\n",
    "\n",
    "    return target_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url_pattern = \"https://climate.northwestknowledge.net/TERRACLIMATE-DATA/TerraClimate_{var}_{year}.nc\"\n",
    "source_urls = []\n",
    "\n",
    "for var in variables:\n",
    "    for year in years:\n",
    "        source_urls.append(source_url_pattern.format(var=var, year=year))\n",
    "source_urls[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads = [download(s, cache_location) for s in source_urls]\n",
    "\n",
    "download_futures = client.compute(downloads, retries=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = [d.result() for d in download_futures]\n",
    "downloaded_files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarrs = [nc2zarr(s, cache_location) for s in downloaded_files]\n",
    "zarr_urls = dask.compute(zarrs, retries=1, scheduler=\"single-threaded\")\n",
    "zarr_urls[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = []\n",
    "for var in variables:\n",
    "    temp = []\n",
    "    for year in tqdm(years):\n",
    "        mapper = fsspec.get_mapper(\n",
    "            f\"gs://carbonplan-scratch/terraclimate-cache/TerraClimate_{var}_{year}.nc.zarr\"\n",
    "        )\n",
    "        temp.append(xr.open_zarr(mapper, consolidated=True))\n",
    "    print(f\"concat {var}\")\n",
    "    ds_list.append(\n",
    "        xr.concat(temp, dim=\"time\", coords=\"minimal\", compat=\"override\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "options.worker_cores = 4\n",
    "options.worker_memory = 16\n",
    "cluster = gateway.new_cluster(cluster_options=options)\n",
    "cluster.adapt(minimum=1, maximum=40)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "ds = xr.merge(ds_list, compat=\"override\").chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(target_location)\n",
    "task = ds.to_zarr(mapper, mode=\"w\", compute=False)\n",
    "dask.compute(task, retries=4)\n",
    "zarr.consolidate_metadata(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
